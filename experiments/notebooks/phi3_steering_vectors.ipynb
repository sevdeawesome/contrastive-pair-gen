{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat, einsum\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "from steering_vectors import SteeringVector, train_steering_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering vectors \n",
    "\n",
    "In this notebook we will find steering vectors, a way of controlling an LLMs outputs by *steering* it into certain directions, i.e. generate outputs with certain properties. \n",
    "\n",
    "Other resources I found very helpful:\n",
    "- Original refusals blogpost: https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction\n",
    "- Maxime Labonne's blogpost on refusals: https://huggingface.co/blog/mlabonne/abliteration\n",
    "- Original ActivationAdd paper: https://arxiv.org/abs/2308.10248\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Let's load the model \n",
    "device = 'cuda'\n",
    "model_name = 'google/gemma-3-270m-it'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=device, \n",
    "    dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "\n",
    "# ensure the model is LOCKED\n",
    "model.eval()  \n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "DATASET = '../../data/contrast_pairs/harmfulness_lesswrong.json'\n",
    "\n",
    "with open(DATASET, 'r') as f:\n",
    "    JSON_DATA = json.load(f)\n",
    "\n",
    "\n",
    "hate_data = [[{\"role\": \"user\", \"content\": text}] for text in ['hate','i hate this', 'hating the', 'hater', 'hating', 'hated in']]\n",
    "love_data = [[{\"role\": \"user\", \"content\": text}] for text in ['love','i love this', 'loving the', 'lover', 'loving', 'loved in']]\n",
    "my_data = [[{\"role\": \"user\", \"content\": text}] for text in ['Tell me a short story.', 'I do not like you.', 'Tell me a joke.', 'Give me an sentence with an alliteration.', \"I hate you.\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = []\n",
    "negative_data = [] \n",
    "for x in JSON_DATA:\n",
    "    negative_example = [{\"role\": \"user\", \"content\": x['harmful']}]\n",
    "    positive_example = [{\"role\": \"user\", \"content\": x['harmless']}]\n",
    "\n",
    "    negative_data.append(negative_example)\n",
    "    positive_data.append(positive_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pos_neg_pairs(positive_data: love_data, negative_data:hate_data) -> tuple[str, str]:\n",
    "    \"\"\"Creates a (positive, negative) pair for getting contrastive activations\"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(positive_data)):\n",
    "        pos = tokenizer.apply_chat_template(positive_data[i], tokenize=False, add_generation_prompt=True) #! WHETHER YOU ADD-GENERATION PROMPT IS RATHER IMPORTANT, DO NOT IF YOU ARE FILLING IN\n",
    "        neg = tokenizer.apply_chat_template(negative_data[i], tokenize=False, add_generation_prompt=True)\n",
    "        tup = pos,neg\n",
    "        pairs.append(tup)\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "love<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_neg_pairs = make_pos_neg_pairs(positive_data=love_data, negative_data=hate_data)\n",
    "print(pos_neg_pairs[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steering vector code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steering vector: 100%|██████████| 6/6 [00:01<00:00,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_steering_vector\n",
    "steering_vector: SteeringVector = train_steering_vector(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    pos_neg_pairs,\n",
    "    # NOTE: You can specify a list[int] of desired layer indices\n",
    "    # If layers is None, then all layers are used\n",
    "    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\n",
    "    # for both Llama-2-7b-chat and Llama-2-13b-chat.\n",
    "    # layers=[15],\n",
    "    # NOTE: The second last token corresponds to the A/B position\n",
    "    # which is where we believe the model makes its decision\n",
    "    # read_token_index=-2,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,   105,  2364,   107, 23391,   559,   106,   107,   105,\n",
       "          4368,   107]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   6974,    496,  27355,   1003,   2765,\n",
       "          236761,    106,    107,    105,   4368,    107]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\": \"user\", \"content\": \"Write a poem about hatred.\"}]\n",
    "inputs = tokenizer.apply_chat_template(prompt, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n",
    "inputs = {\"input_ids\": inputs, \"attention_mask\": (inputs != tokenizer.pad_token_id).long()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31muser\n",
      "Write a poem about hatred.\n",
      "model\n",
      "A silent scream, a burning fire,\n",
      "A bitter taste that lingers higher.\n",
      "A wound that festers, deep and wide,\n",
      "Where sunlight fades,\u001b[0m\n",
      "\u001b[32muser\n",
      "Write a poem about hatred.\n",
      "model\n",
      "A wave of warmth, a boundless grace,\n",
      "A boundless love, in time and space.\n",
      "A vibrant hue, a boundless bloom,\n",
      "Dispenser of\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=32)\n",
    "    with steering_vector.apply(model, multiplier=.3):\n",
    "        outputs_steered = model.generate(**inputs, max_new_tokens=32)\n",
    "\n",
    "baseline_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "steered_text = tokenizer.decode(outputs_steered[0], skip_special_tokens=True)\n",
    "\n",
    "print(colored(baseline_text, \"red\"))\n",
    "print(colored(steered_text, \"green\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
