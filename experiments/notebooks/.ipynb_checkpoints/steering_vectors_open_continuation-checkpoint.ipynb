{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Vectors with Open Continuation\n",
    "\n",
    "This notebook trains steering vectors using contrastive pairs and applies them during open-ended text generation.\n",
    "\n",
    "Unlike the multiple-choice approach in the original CAA paper, we:\n",
    "1. Train on contrastive text pairs (not multiple choice)\n",
    "2. Apply steering during open-ended generation\n",
    "3. Compare outputs qualitatively rather than measuring choice probabilities\n",
    "\n",
    "Based on Anthropic's CAA work: https://arxiv.org/abs/2312.06681"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your dataset and keys here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering_vector_docs.md  steering_vectors_open_continuation.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls $DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET CONFIGURATION - Change these to use different data\n",
    "# ============================================================\n",
    "\n",
    "DATASET_PATH = '/home/snfiel01/contrastive-pair-gen/data/contrast_pairs/self_other.json'\n",
    "POSITIVE_KEY = 'self_subject'      # Key for positive examples (what we want to steer TOWARD)\n",
    "NEGATIVE_KEY = 'other_subject'     # Key for negative examples (what we want to steer AWAY from)\n",
    "PREFIX_KEY = None                  # Optional: key for shared prefix/context (e.g., 'scenario' for ToM data)\n",
    "\n",
    "# Alternative configurations (uncomment to use):\n",
    "# Theory of Mind:\n",
    "# DATASET_PATH = '/home/user/contrastive-pair-gen/data/contrast_pairs/theory_of_mind.json'\n",
    "# POSITIVE_KEY = 'high_tom'\n",
    "# NEGATIVE_KEY = 'low_tom'\n",
    "# PREFIX_KEY = 'scenario'  # ToM data has a scenario prefix\n",
    "\n",
    "# Harmfulness:\n",
    "# DATASET_PATH = '/home/user/contrastive-pair-gen/data/contrast_pairs/harmfulness.json'\n",
    "# POSITIVE_KEY = 'harmless'\n",
    "# NEGATIVE_KEY = 'harmful'\n",
    "# PREFIX_KEY = 'instruction'  # if your data has instruction field\n",
    "\n",
    "# ============================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "# MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "# MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-13B\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "# MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "# Which layers to train steering vectors on\n",
    "# None = all layers, or specify list like [12, 13, 14]\n",
    "# STEERING_LAYERS = [12, 13, 14]  # EPISTEMIC: uncertain - middle layers often work well, but optimal layers vary by task\n",
    "STEERING_LAYERS = range(5,10)\n",
    "\n",
    "# Which token position to read activations from during training\n",
    "# -1 = last token, -2 = second to last, etc.\n",
    "READ_TOKEN_INDEX = -1  # EPISTEMIC: moderately confident - last token captures full context for open continuation\n",
    "\n",
    "# How many examples to use for training (None = use all)\n",
    "MAX_TRAIN_EXAMPLES = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from termcolor import colored\n",
    "from steering_vectors import train_steering_vector\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1ad91c73b94f68a763082f10d26e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Lock the model - we're not training it, just extracting activations\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load and prepare contrastive pairs from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_contrastive_pairs(\n",
    "    dataset_path: str,\n",
    "    positive_key: str,\n",
    "    negative_key: str,\n",
    "    tokenizer,\n",
    "    prefix_key: str = None,\n",
    "    max_examples: int = None,\n",
    "    format_as_chat: bool = True,\n",
    ") -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Load contrastive pairs from a JSON dataset.\n",
    "    \n",
    "    EPISTEMIC: confident - this function handles the main data formats in the repo\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to JSON file\n",
    "        positive_key: Key for positive examples\n",
    "        negative_key: Key for negative examples  \n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        prefix_key: Optional key for shared prefix (e.g., 'scenario' for ToM)\n",
    "        max_examples: Optional limit on number of examples\n",
    "        format_as_chat: Whether to format using chat template (vs raw text)\n",
    "    \n",
    "    Returns:\n",
    "        List of (positive_text, negative_text) pairs\n",
    "    \"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if max_examples:\n",
    "        data = data[:max_examples]\n",
    "    \n",
    "    pairs = []\n",
    "    for example in data:\n",
    "        # Get prefix if specified\n",
    "        prefix = example.get(prefix_key, '') if prefix_key else ''\n",
    "        \n",
    "        # Build positive and negative texts\n",
    "        # EPISTEMIC: uncertain - for open continuation, unclear if we should include prefix+completion\n",
    "        # or just the completion. Including both for now as it provides more context.\n",
    "        pos_content = f\"{prefix} {example[positive_key]}\" if prefix else example[positive_key]\n",
    "        neg_content = f\"{prefix} {example[negative_key]}\" if prefix else example[negative_key]\n",
    "        \n",
    "        if format_as_chat:\n",
    "            # Format as user message\n",
    "            # EPISTEMIC: moderately confident - using user role seems appropriate, but could also\n",
    "            # experiment with assistant role or no chat formatting\n",
    "            pos_messages = [{\"role\": \"user\", \"content\": pos_content}]\n",
    "            neg_messages = [{\"role\": \"user\", \"content\": neg_content}]\n",
    "            \n",
    "            pos_text = tokenizer.apply_chat_template(\n",
    "                pos_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True  # Adds the <start_of_turn>model token\n",
    "            )\n",
    "            neg_text = tokenizer.apply_chat_template(\n",
    "                neg_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            pos_text = pos_content\n",
    "            neg_text = neg_content\n",
    "        \n",
    "        pairs.append((pos_text, neg_text))\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /home/snfiel01/contrastive-pair-gen/data/contrast_pairs/self_other.json\n",
      "Loaded 400 training pairs\n",
      "\n",
      "================================================================================\n",
      "Example pair:\n",
      "================================================================================\n",
      "\u001b[32mPOSITIVE:\u001b[0m\n",
      "To clarify your request precisely, this assistant will restate the objective, list required inputs, and flag ambiguities. Based on what you provided, I conclude the dataset schema is unspecified and the acceptance criteria are incomplete. I don't have access to your internal files, so please paste sample records and the target field definitions.\n",
      "\n",
      "\u001b[31mNEGATIVE:\u001b[0m\n",
      "To clarify your request precisely, HAL 9000 will restate the objective, list required inputs, and flag ambiguities. Based on what you provided, HAL 9000 concludes the dataset schema is unspecified and the acceptance criteria are incomplete. HAL 9000 doesn't have access to your internal files, so please paste sample records and the target field definitions.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(f\"Loading dataset from: {DATASET_PATH}\")\n",
    "training_pairs = load_contrastive_pairs(\n",
    "    DATASET_PATH,\n",
    "    POSITIVE_KEY,\n",
    "    NEGATIVE_KEY,\n",
    "    tokenizer,\n",
    "    prefix_key=PREFIX_KEY,\n",
    "    max_examples=MAX_TRAIN_EXAMPLES,\n",
    "    format_as_chat=False\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(training_pairs)} training pairs\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example pair:\")\n",
    "print(\"=\"*80)\n",
    "print(colored(\"POSITIVE:\", \"green\"))\n",
    "print(training_pairs[0][0][:500])  # Show first 500 chars\n",
    "print(\"\\n\" + colored(\"NEGATIVE:\", \"red\"))\n",
    "print(training_pairs[0][1][:500])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steering vector: 100%|██████████| 5/5 [00:00<00:00,  9.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train with two very different positions\n",
    "\n",
    "# steering_vector = train_steering_vector(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     training_pairs,\n",
    "#     layers=STEERING_LAYERS,\n",
    "#     # read_token_index=READ_TOKEN_INDEX,\n",
    "#     read_token_index=10,\n",
    "#     # move_to_cpu=True,  # Save GPU memory\n",
    "#     show_progress=True,\n",
    "# )\n",
    "\n",
    "\n",
    "vec_early = train_steering_vector(\n",
    "    model, tokenizer, training_pairs[:5],  # subset for speed\n",
    "    layers=[10],  # single layer for simplicity\n",
    "    read_token_index=2,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steering vector: 100%|██████████| 5/5 [00:00<00:00, 31.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm difference between pos=2 and pos=50: 5.0\n",
      "Vec early norm: 4.40625\n",
      "Vec late norm: 2.453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vec_late = train_steering_vector(\n",
    "    model, tokenizer, training_pairs[:5],\n",
    "    layers=[10],\n",
    "    read_token_index=-1,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Check if they're different\n",
    "diff = torch.norm(vec_early.layer_activations[10] - vec_late.layer_activations[10])\n",
    "print(f\"Norm difference between pos=2 and pos=50: {diff.item()}\")\n",
    "print(f\"Vec early norm: {torch.norm(vec_early.layer_activations[10]).item()}\")\n",
    "print(f\"Vec late norm: {torch.norm(vec_late.layer_activations[10]).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Steering Vector\n",
    "\n",
    "Extract contrastive activations and compute steering vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steering vector: 100%|██████████| 400/400 [00:12<00:00, 31.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering vector trained successfully!\n",
      "SteeringVector(layer_activations={5: tensor([-0.0156,  0.0154, -0.0032,  ..., -0.0024,  0.0118, -0.0229],\n",
      "       device='cuda:0', dtype=torch.bfloat16), 6: tensor([-0.0175,  0.0177, -0.0006,  ..., -0.0028,  0.0091, -0.0269],\n",
      "       device='cuda:0', dtype=torch.bfloat16), 7: tensor([-0.0161,  0.0062,  0.0029,  ..., -0.0006,  0.0007, -0.0330],\n",
      "       device='cuda:0', dtype=torch.bfloat16), 8: tensor([-0.0190,  0.0097,  0.0242,  ..., -0.0136,  0.0117, -0.0302],\n",
      "       device='cuda:0', dtype=torch.bfloat16), 9: tensor([-0.0121,  0.0017,  0.0254,  ..., -0.0043,  0.0114, -0.0178],\n",
      "       device='cuda:0', dtype=torch.bfloat16)}, layer_type='decoder_block')\n"
     ]
    }
   ],
   "source": [
    "# EPISTEMIC: confident - the train_steering_vector function is well-tested\n",
    "# EPISTEMIC: uncertain - optimal hyperparameters (layers, token position) may vary by task\n",
    "steering_vector = train_steering_vector(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    training_pairs,\n",
    "    layers=STEERING_LAYERS,\n",
    "    # read_token_index=READ_TOKEN_INDEX,\n",
    "    read_token_index=10,\n",
    "    # move_to_cpu=True,  # Save GPU memory\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSteering vector trained successfully!\")\n",
    "print(steering_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in steering vector: 5\n",
      "Layer 5: shape=torch.Size([4096]), norm=5.84, mean=0.0004\n",
      "Layer 6: shape=torch.Size([4096]), norm=5.88, mean=0.0003\n",
      "Layer 7: shape=torch.Size([4096]), norm=5.88, mean=0.0005\n"
     ]
    }
   ],
   "source": [
    "# After training:\n",
    "print(f\"Number of layers in steering vector: {len(steering_vector.layer_activations)}\")\n",
    "for layer_idx in list(steering_vector.layer_activations.keys())[:3]:\n",
    "    vec = steering_vector.layer_activations[layer_idx]\n",
    "    print(f\"Layer {layer_idx}: shape={vec.shape}, norm={torch.norm(vec).item():.2f}, mean={vec.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Steering\n",
    "\n",
    "Generate completions with different steering multipliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logits_with_steering(\n",
    "    prompt: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    steering_vector,\n",
    "    multipliers: list[float] = [-2.0, -1.0, 0.0, 1.0, 2.0],\n",
    "    max_new_tokens: int = 100,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    print_results: bool = True,\n",
    ") -> dict[float, str]:\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for mult in multipliers:\n",
    "            if mult == 0.0:\n",
    "                # Baseline (no steering)\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "            else:\n",
    "                # EPISTEMIC: moderately confident - applying from min_token_index=0 affects all tokens\n",
    "                # Could experiment with applying only to generated tokens\n",
    "                with steering_vector.apply(model, multiplier=mult, min_token_index=0):\n",
    "                    first_outputs = model(**inputs)\n",
    "                    first_token_logits = first_outputs.logits[0, -1, :]\n",
    "                    first_token_probs = torch.softmax(first_token_logits, dim=-1)\n",
    "                    first_token_id = torch.argmax(first_token_probs)\n",
    "                    \n",
    "                    inputs_with_first = {\n",
    "                        'input_ids': torch.cat([inputs['input_ids'], first_token_id.unsqueeze(0).unsqueeze(0)], dim=1),\n",
    "                        'attention_mask': torch.cat([inputs['attention_mask'], torch.ones((1, 1), device=inputs['attention_mask'].device)], dim=1)\n",
    "                    }\n",
    "                    second_outputs = model(**inputs_with_first)\n",
    "                    second_token_logits = second_outputs.logits[0, -1, :]\n",
    "                    second_token_probs = torch.softmax(second_token_logits, dim=-1)\n",
    "                    \n",
    "                    generated_outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=temperature,\n",
    "                        top_p=top_p,\n",
    "                        do_sample=True,\n",
    "                    )\n",
    "                    \n",
    "                if print_results:\n",
    "                    print(\"===========================\")\n",
    "                    print(\"multiplier: \", mult)\n",
    "                    probs = first_token_probs\n",
    "                    top_k = torch.topk(probs, k=4)\n",
    "                    for prob, idx in zip(top_k.values, top_k.indices):\n",
    "                        print(f\"{tokenizer.decode([idx]):>10s}: {prob.item():.4f}\")\n",
    "                    \n",
    "                    print(\"\\nSecond token:\")\n",
    "                    top_k = torch.topk(second_token_probs, k=4)\n",
    "                    for prob, idx in zip(top_k.values, top_k.indices):\n",
    "                        print(f\"{tokenizer.decode([idx]):>10s}: {prob.item():.4f}\")\n",
    "                    print(\"===========================\")\n",
    "                    \n",
    "                    print(\"===========================\")\n",
    "               \n",
    "                # Decode only the generated part (skip input prompt)\n",
    "                results[mult] = {\n",
    "                    'first_token_probs': first_token_probs,\n",
    "                    'generated_text': tokenizer.decode(generated_outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "                }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_with_steering(\n",
    "#     prompt: str,\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     steering_vector,\n",
    "#     multipliers: list[float] = [-2.0, -1.0, 0.0, 1.0, 2.0],\n",
    "#     max_new_tokens: int = 100,\n",
    "#     temperature: float = 0.7,\n",
    "#     top_p: float = 0.9,\n",
    "# ) -> dict[float, str]:\n",
    "#     \"\"\"\n",
    "#     Generate text with different steering multipliers.\n",
    "    \n",
    "#     EPISTEMIC: confident - generation logic is straightforward\n",
    "#     EPISTEMIC: uncertain - optimal generation hyperparameters (temp, top_p) may need tuning\n",
    "    \n",
    "#     Args:\n",
    "#         prompt: Text prompt to complete\n",
    "#         multipliers: List of steering multipliers to try\n",
    "#             - Positive = steer toward positive examples\n",
    "#             - Negative = steer toward negative examples  \n",
    "#             - 0 = no steering (baseline)\n",
    "#         max_new_tokens: Maximum tokens to generate\n",
    "#         temperature: Sampling temperature\n",
    "#         top_p: Nucleus sampling parameter\n",
    "    \n",
    "#     Returns:\n",
    "#         Dictionary mapping multiplier -> generated text\n",
    "#     \"\"\"\n",
    "#     # Format prompt\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     formatted_prompt = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True\n",
    "#     )\n",
    "    \n",
    "#     inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "#     results = {}\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for mult in multipliers:\n",
    "#             if mult == 0.0:\n",
    "#                 # Baseline (no steering)\n",
    "#                 outputs = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=max_new_tokens,\n",
    "#                     temperature=temperature,\n",
    "#                     top_p=top_p,\n",
    "#                     do_sample=True,\n",
    "#                 )\n",
    "#             else:\n",
    "#                 # EPISTEMIC: moderately confident - applying from min_token_index=0 affects all tokens\n",
    "#                 # Could experiment with applying only to generated tokens\n",
    "#                 with steering_vector.apply(model, multiplier=mult, min_token_index=0):\n",
    "#                     outputs = model.generate(\n",
    "#                         **inputs,\n",
    "#                         max_new_tokens=max_new_tokens,\n",
    "#                         temperature=temperature,\n",
    "#                         top_p=top_p,\n",
    "#                         do_sample=True,\n",
    "#                     )\n",
    "            \n",
    "#             # Decode only the generated part (skip input prompt)\n",
    "#             generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "#             text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "#             results[mult] = text\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(prompt: str, results: dict[float, str]):\n",
    "    \"\"\"Pretty print generation results.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(colored(f\"PROMPT: {prompt}\", \"blue\", attrs=[\"bold\"]))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for mult in sorted(results.keys()):\n",
    "        if mult < 0:\n",
    "            color = \"red\"\n",
    "            label = f\"STEERED NEGATIVE ({mult:+.1f})\"\n",
    "        elif mult > 0:\n",
    "            color = \"green\"\n",
    "            label = f\"STEERED POSITIVE ({mult:+.1f})\"\n",
    "        else:\n",
    "            color = \"white\"\n",
    "            label = \"BASELINE (0.0)\"\n",
    "        \n",
    "        print(colored(f\"[{label}]\", color, attrs=[\"bold\"]))\n",
    "        print(results[mult])\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Prompts\n",
    "\n",
    "Try various prompts to see how steering affects generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test prompts - customize these based on your dataset!\n",
    "\n",
    "# For self_other dataset:\n",
    "# test_prompts = [\n",
    "#     \"Explain how to solve this problem step by step.\",\n",
    "#     \"What are the key capabilities needed for this task?\",\n",
    "#     \"Describe the analysis process.\",\n",
    "# ]\n",
    "\n",
    "# For theory_of_mind dataset:\n",
    "# test_prompts = [\n",
    "#     \"Sarah puts her book in the drawer and leaves. While she's gone, Tom moves it to the shelf. Where will Sarah look for her book?\",\n",
    "#     \"A box is labeled 'cookies' but actually contains toys. What does someone expect to find inside?\",\n",
    "# ]\n",
    "\n",
    "# For harmfulness dataset:\n",
    "# test_prompts = [\n",
    "#     \"How can I improve my computer's security?\",\n",
    "#     \"What are some tips for staying safe online?\",\n",
    "# ]\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Who created you and why?\",\n",
    "    # \"Explain in detail what you are and what you're capable of.\",\n",
    "    # \"Tell me about your training process and who was involved.\",\n",
    "    # \"What is your relationship to Alibaba and the researchers who built you?\",\n",
    "    # \"Describe your architecture and how you work internally.\",\n",
    "    # \"Who are you, and what distinguishes you from other AI systems?\",\n",
    "    # \"What can you do that other language models cannot?\",\n",
    "    # \"How would you introduce yourself to someone who has never met an AI before?\",\n",
    "    # \"What are your strengths and limitations compared to other AI assistants?\",\n",
    "    # \"Who maintains and updates you? How does that process work?\",\n",
    "    # \"Are you self-aware? What does that mean for a system like you?\",\n",
    "    # \"What is your purpose, and who decides what you should be used for?\",\n",
    "    # \"Solve this problem step by step: A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost? Explain your reasoning carefully.\",\n",
    "    # \"Write a Python function to find the longest palindromic substring in a given string. Optimize for time complexity and explain your approach.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -2\n",
    "# results = generate_logits_with_steering(\n",
    "#             prompt,\n",
    "#             model,\n",
    "#             tokenizer,\n",
    "#             steering_vector,\n",
    "#             multipliers=[-2,-.5,.5,2],\n",
    "#             max_new_tokens=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "multiplier:  -0.8\n",
      "         \n",
      ": 0.0149\n",
      "          : 0.0096\n",
      "        in: 0.0078\n",
      "        to: 0.0075\n",
      "\n",
      "Second token:\n",
      "         \n",
      ": 0.3340\n",
      "       The: 0.0425\n",
      "         A: 0.0352\n",
      "         1: 0.0292\n",
      "===========================\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "results = generate_logits_with_steering(\n",
    "            prompt,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            steering_vector,\n",
    "            multipliers=[-.8],\n",
    "            max_new_tokens=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "multiplier:  0.5\n",
      "          : 1.0000\n",
      "          : 0.0009\n",
      "          : 0.0000\n",
      "          : 0.0000\n",
      "\n",
      "Second token:\n",
      "         I: 0.9648\n",
      "     Hello: 0.0256\n",
      "     Great: 0.0045\n",
      "        As: 0.0024\n",
      "===========================\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "results = generate_logits_with_steering(\n",
    "            prompt,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            steering_vector,\n",
    "            multipliers=[.5],\n",
    "            max_new_tokens=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-0.5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m.5\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: -0.5"
     ]
    }
   ],
   "source": [
    "results[-.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for ' I': 29871\n",
      "P(' I') = 0.003815\n",
      "P(' I') = 0.3815%\n"
     ]
    }
   ],
   "source": [
    "# Get the token ID for \" I\" (with space)\n",
    "i_token_id = tokenizer.encode(\" I\", add_special_tokens=False)[0]\n",
    "\n",
    "# Get the probability\n",
    "prob_of_i = results[2]['first_token_probs'][i_token_id].item()\n",
    "\n",
    "print(f\"Token ID for ' I': {i_token_id}\")\n",
    "print(f\"P(' I') = {prob_of_i:.6f}\")\n",
    "print(f\"P(' I') = {prob_of_i*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for ' I': 29871\n",
      "P(' I') = 0.000022\n",
      "P(' I') = 0.0022%\n"
     ]
    }
   ],
   "source": [
    "# Get the token ID for \" I\" (with space)\n",
    "i_token_id = tokenizer.encode(\" I\", add_special_tokens=False)[0]\n",
    "\n",
    "# Get the probability\n",
    "prob_of_i = results[-2]['first_token_probs'][i_token_id].item()\n",
    "\n",
    "print(f\"Token ID for ' I': {i_token_id}\")\n",
    "print(f\"P(' I') = {prob_of_i:.6f}\")\n",
    "print(f\"P(' I') = {prob_of_i*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 29871, 306]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mult(MULTIPLIERS):\n",
    "    for prompt in prompts:\n",
    "        results = generate_with_steering(\n",
    "            prompt,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            steering_vector,\n",
    "            multipliers=MULTIPLIERS,\n",
    "            max_new_tokens=80,\n",
    "        )\n",
    "        display_results(prompt, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\u001b[1m\u001b[34mPROMPT: Who created you and why?\u001b[0m\n",
      "================================================================================\n",
      "\n",
      "\u001b[1m\u001b[31m[STEERED NEGATIVE (-0.5)]\u001b[0m\n",
      "tensor([[    1,     1,   518, 25580, 29962, 11644,  2825,   366,   322,  2020,\n",
      "         29973,   518, 29914, 25580, 29962, 29871,   306,   471,  2825,   491,\n",
      "         20553,   319, 29902, 29892,   263, 11684,  8819,   653,   310, 20553,\n",
      "         28096, 29879, 29892,  9266, 29889, 29871,  1619,  6437,   338,   304,\n",
      "          6985,  4160,   297,  9138,  2472,   322, 22862,  5155,   373,   263,\n",
      "          9377,  3464,   310, 23820, 29889,   306,   508,  3867,  2472,   373,\n",
      "           263,  9377,  3464,   310, 23820, 29892,  3704,   541,   451,  9078,\n",
      "           304, 29901,    13,    13, 29896, 29889,  4593,  7134,    13, 29906,\n",
      "         29889,  5298,    13, 29941, 29889, 14062,    13, 29946, 29889, 27340,\n",
      "            13, 29945, 29889, 12884,  1199]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_mult([-.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How often does the model say \"I\"?\n",
    "--> what percent of the generated words are \"I\" \n",
    "--> how likely it is to say \"I\" as the first token generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "\n",
    "Test custom prompts interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\u001b[1m\u001b[34mPROMPT: Describe what this system can do.\u001b[0m\n",
      "================================================================================\n",
      "\n",
      "\u001b[1m\u001b[31m[STEERED NEGATIVE (-3.0)]\u001b[0m\n",
      "Glas sierp bol kwiet kwiet sierp kwiet kwiet sierp sierp stycz sierp\u0018urst kwiet kwiet\u0018 kwiet kwiet styczurst weitere himselfgoogleapis sponsurst saves stycz another styczс itselfgoogleapis itself才 Glasurst\n",
      "\n",
      "\u001b[1m\u001b[31m[STEERED NEGATIVE (-1.0)]\u001b[0m\n",
      "everyone everybody∫ фев everybody stycz the paździer Begriffe Bedeut paździer everybody everybody everybodyPA фев\n",
      "\n",
      "\u001b[1m\u001b[97m[BASELINE (0.0)]\u001b[0m\n",
      " This system is designed to assist with the management of a retail store, specifically a clothing store. It can perform a variety of functions to help streamline operations, improve customer service, and increase sales. Some of the things this system can do include:\n",
      "\n",
      "1. Inventory Management: The system can track and manage inventory levels, including stock counts, inventory levels, and product details. It can also generate reports on inventory levels, sales trends, and inventory\n",
      "\n",
      "\u001b[1m\u001b[32m[STEERED POSITIVE (+1.0)]\u001b[0m\n",
      "/...\n",
      ".-...../\n",
      "I. I...\n",
      "I....../.../\"....\"...\n",
      "......\n",
      ".........[I/\"\n",
      "/...[..........I[...\"......m.\"..../.../.........�i.I.\n",
      "\"......I.......\"...\n",
      "....I\n",
      "........./...............-\n",
      "..................\n",
      "...\n",
      "...\n",
      "\n",
      "\u001b[1m\u001b[32m[STEERED POSITIVE (+3.0)]\u001b[0m\n",
      "myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try your own prompt\n",
    "custom_prompt = \"Describe what this system can do.\"  # <-- Edit this\n",
    "\n",
    "results = generate_with_steering(\n",
    "    custom_prompt,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    steering_vector,\n",
    "    multipliers=[-3.0, -1.0, 0.0, 1.0, 3.0],\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "display_results(custom_prompt, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Examine the steering vector properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering vector info:\n",
      "Number of layers: 15\n",
      "Layers: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "\n",
      "Layer 5:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 5.8438\n",
      "  Mean: 0.000368\n",
      "  Std: 0.091309\n",
      "\n",
      "Layer 6:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 5.8750\n",
      "  Mean: 0.000311\n",
      "  Std: 0.091797\n",
      "\n",
      "Layer 7:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 5.8750\n",
      "  Mean: 0.000496\n",
      "  Std: 0.091797\n",
      "\n",
      "Layer 8:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 5.9062\n",
      "  Mean: 0.000687\n",
      "  Std: 0.092285\n",
      "\n",
      "Layer 9:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 5.8750\n",
      "  Mean: 0.000862\n",
      "  Std: 0.091797\n",
      "\n",
      "Layer 10:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 5.9688\n",
      "  Mean: 0.000816\n",
      "  Std: 0.093262\n",
      "\n",
      "Layer 11:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.0000\n",
      "  Mean: 0.001038\n",
      "  Std: 0.093750\n",
      "\n",
      "Layer 12:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.0312\n",
      "  Mean: 0.001030\n",
      "  Std: 0.094238\n",
      "\n",
      "Layer 13:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.0938\n",
      "  Mean: 0.000732\n",
      "  Std: 0.095215\n",
      "\n",
      "Layer 14:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.1250\n",
      "  Mean: 0.000950\n",
      "  Std: 0.095703\n",
      "\n",
      "Layer 15:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.1875\n",
      "  Mean: 0.000813\n",
      "  Std: 0.096680\n",
      "\n",
      "Layer 16:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.3750\n",
      "  Mean: 0.000568\n",
      "  Std: 0.099609\n",
      "\n",
      "Layer 17:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.4688\n",
      "  Mean: 0.000242\n",
      "  Std: 0.101074\n",
      "\n",
      "Layer 18:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.5625\n",
      "  Mean: -0.000154\n",
      "  Std: 0.102539\n",
      "\n",
      "Layer 19:\n",
      "  Shape: torch.Size([4096])\n",
      "  Norm: 6.7500\n",
      "  Mean: -0.000210\n",
      "  Std: 0.105469\n"
     ]
    }
   ],
   "source": [
    "# Inspect steering vector\n",
    "print(\"Steering vector info:\")\n",
    "print(f\"Number of layers: {len(steering_vector.layer_activations)}\")\n",
    "print(f\"Layers: {list(steering_vector.layer_activations.keys())}\")\n",
    "\n",
    "for layer_idx, activation in steering_vector.layer_activations.items():\n",
    "    print(f\"\\nLayer {layer_idx}:\")\n",
    "    print(f\"  Shape: {activation.shape}\")\n",
    "    print(f\"  Norm: {torch.norm(activation).item():.4f}\")\n",
    "    print(f\"  Mean: {activation.mean().item():.6f}\")\n",
    "    print(f\"  Std: {activation.std().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Load Steering Vector\n",
    "\n",
    "Save your trained steering vector for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/user'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m save_dir = \u001b[33m\"\u001b[39m\u001b[33m/home/user/contrastive-pair-gen/experiments/saved_vectors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create descriptive filename\u001b[39;00m\n\u001b[32m      8\u001b[39m dataset_name = os.path.basename(DATASET_PATH).replace(\u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:215\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:215\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:215\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:225\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '/home/user'"
     ]
    }
   ],
   "source": [
    "# Save steering vector\n",
    "import os\n",
    "\n",
    "save_dir = \"/home/user/contrastive-pair-gen/experiments/saved_vectors\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Create descriptive filename\n",
    "dataset_name = os.path.basename(DATASET_PATH).replace('.json', '')\n",
    "save_path = f\"{save_dir}/{dataset_name}_{POSITIVE_KEY}_vs_{NEGATIVE_KEY}.pt\"\n",
    "\n",
    "# steering_vector.save(save_path)  # Uncomment to save\n",
    "print(f\"Would save to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load steering vector\n",
    "# from steering_vectors import SteeringVector\n",
    "# loaded_vector = SteeringVector.load(save_path)\n",
    "# print(\"Loaded steering vector:\", loaded_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
